{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anegi/anaconda3/envs/marker_env/lib/python3.12/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.21). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "import json \n",
    "from PIL import Image\n",
    "import random \n",
    "import time \n",
    "import cv2 \n",
    "import math \n",
    "import albumentations as A \n",
    "from scipy.spatial.transform import Rotation as R \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from math import gcd, lcm\n",
    "from random import randint, uniform\n",
    "from random import random as random_function \n",
    "from math import cos, sin, radians \n",
    "from perlin_numpy import generate_fractal_noise_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_point_to_image(C,T,P): \n",
    "    P_H = np.array([[P[0]],[P[1]],[P[2]],[1]]) \n",
    "    T_H = T[:3,:4]  \n",
    "    uv = C @ T_H @ P_H \n",
    "    if uv[2] != 0: \n",
    "        uv = uv / uv[2] # NOTE: check if the alternative case of not dividing when z=0 is valid \n",
    "    uv = uv[:2] \n",
    "    uv = uv.reshape((2)) \n",
    "    return uv \n",
    "\n",
    "def project_point_list_to_image(C,T,P_list): \n",
    "    uv_list = []  \n",
    "    for P in P_list: \n",
    "        uv = project_point_to_image(C,T,P) \n",
    "        uv_list.append(uv) \n",
    "    return uv_list   \n",
    "\n",
    "def transform_pts(pts, T):  \n",
    "    pts_transformed = [] \n",
    "    for pt in pts: \n",
    "        pt = pt.reshape(3,1) \n",
    "        pt = np.vstack((pt, 1))  \n",
    "        pt_transformed = T @ pt  \n",
    "        pts_transformed.append(pt_transformed[:3]) \n",
    "    return pts_transformed\n",
    "\n",
    "def overlay_points_on_image(image, pixel_points, radius=5, color=(0, 0, 255), thickness=-1):\n",
    "    \"\"\"\n",
    "    Overlays a list of pixel points on the input image.\n",
    "\n",
    "    Parameters:\n",
    "    - image: The input image (a NumPy array).\n",
    "    - pixel_points: A list of 2D pixel coordinates [(x1, y1), (x2, y2), ...].\n",
    "    - radius: The radius of the circle to draw around each point. Default is 5.\n",
    "    - color: The color of the circle (BGR format). Default is red (0, 0, 255).\n",
    "    - thickness: The thickness of the circle. Default is -1 to fill the circle.\n",
    "\n",
    "    Returns:\n",
    "    - The image with points overlaid.\n",
    "    \"\"\"\n",
    "    # Iterate over each pixel point and overlay it on the image\n",
    "    for point in pixel_points:\n",
    "        if point is not None:  # Only overlay valid points\n",
    "            x, y = int(point[0]), int(point[1])\n",
    "            # check if the point is within the image bounds\n",
    "            if x < 0 or x >= image.shape[1] or y < 0 or y >= image.shape[0]:\n",
    "                continue\n",
    "            # Draw a filled circle at the pixel coordinates\n",
    "            cv2.circle(image, (x, y), radius, color, thickness)\n",
    "    return image\n",
    "\n",
    "def compute_2D_gridpoints(N=10,s=0.1): \n",
    "    # N = num squares, s = side length  \n",
    "    u = np.linspace(-s/2, +s/2, N+1) \n",
    "    v = np.linspace(-s/2, +s/2, N+1) \n",
    "    gridpoints = [] \n",
    "    for uu in u:\n",
    "        for vv in v: \n",
    "            gridpoints.append(np.array([uu,vv,0])) \n",
    "    return gridpoints "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUGMENTATION HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rotate3d(pic, rot_x, rot_y, rot_z, f_mult = 1.0, fill_color = (0,0,0)):\n",
    "\n",
    "    height, width = [(2 * i) for i in pic.shape[0:2]]\n",
    "\n",
    "    pic_exp = np.zeros((height, width, 4), dtype = np.uint8)\n",
    "    pic_exp[:,:,:3] = fill_color\n",
    "    pic_exp[pic.shape[0]//2:(height + pic.shape[0])//2,\n",
    "            pic.shape[1]//2:(width + pic.shape[1])//2, :] = pic\n",
    "\n",
    "    alpha = radians(rot_x)\n",
    "    beta = radians(rot_y)\n",
    "    gamma = radians(rot_z)\n",
    "\n",
    "    f = (width / 2) * f_mult\n",
    "\n",
    "    # 2d -> 3d\n",
    "    proj2d3d = np.asarray([[1, 0, -width / 2],\n",
    "                           [0, 1, -height / 2],\n",
    "                           [0, 0, 0],\n",
    "                           [0, 0, 1]])\n",
    "\n",
    "    # Rotation matrices\n",
    "    rx = np.asarray([[1, 0, 0, 0],\n",
    "                     [0, cos(alpha), -sin(alpha), 0],\n",
    "                     [0, sin(alpha), cos(alpha), 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "    \n",
    "    ry = np.asarray([[cos(beta), 0, sin(beta), 0],\n",
    "                     [0, 1, 0, 0],\n",
    "                     [-sin(beta), 0, cos(beta), 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "    \n",
    "    rz = np.asarray([[cos(gamma), -sin(gamma), 0, 0],\n",
    "                     [sin(gamma), cos(gamma), 0, 0],\n",
    "                     [0, 0, 1, 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "    \n",
    "    # Translation\n",
    "    T = np.asarray([[1, 0, 0, 0],\n",
    "                    [0, 1, 0, 0],\n",
    "                    [0, 0, 1, f],\n",
    "                    [0, 0, 0, 1]])\n",
    "    \n",
    "    # 3d -> 2d\n",
    "    proj3d2d = np.asarray([[f, 0, width / 2, 0],\n",
    "                           [0, f, height / 2, 0],\n",
    "                           [0, 0, 1, 0]])\n",
    "    \n",
    "    # Combine all\n",
    "    transform = proj3d2d @ (T @ ((rx @ ry @ rz) @ proj2d3d))\n",
    "    pic_exp = cv2.warpPerspective(pic_exp, transform, (width, height), borderMode=cv2.BORDER_CONSTANT, borderValue=fill_color)\n",
    "\n",
    "    return pic_exp, transform\n",
    "\n",
    "def gradient(width, height):\n",
    "\n",
    "    t_size = max(width, height)\n",
    "    size = t_size * 2\n",
    "\n",
    "    grad = np.zeros((size, size))\n",
    "\n",
    "    for i in range(size):\n",
    "        grad[i] = (i / size)\n",
    "\n",
    "    center = grad.shape[0]//2\n",
    "    mat = cv2.getRotationMatrix2D((center, center), random_function() * 360, 1.0)\n",
    "    pic = cv2.warpAffine(grad, mat, (size, size))\n",
    "\n",
    "    # Final crop\n",
    "\n",
    "    center = grad.shape[0]//2\n",
    "    pic = pic[center - height//2:center + height//2, center - width//2:center + width//2]\n",
    "\n",
    "    # Re-range\n",
    "\n",
    "    pic = (pic - np.min(pic)) / (np.max(pic) - np.min(pic) + 1e-6)\n",
    "\n",
    "    return pic\n",
    "\n",
    "def lines(width, height, num_patterns = 3):\n",
    "\n",
    "    t_size = max(width, height)\n",
    "    size = t_size * 2\n",
    "\n",
    "    pic = np.ones((size, size))\n",
    "    center = pic.shape[0]//2\n",
    "\n",
    "    for i in range(num_patterns):\n",
    "\n",
    "        curr = 0\n",
    "\n",
    "        while curr < size:\n",
    "            paint = randint(1, max((size - curr)//2, 1))#min(randint(0, 16), size - curr)\n",
    "            skip = randint(1, max((size - curr - paint)//2, 1))#min(randint(0, 16), size - curr - paint)\n",
    "            pic[curr:curr + paint] *= uniform(0.0, 2.0)#random()\n",
    "            curr = curr + paint + skip\n",
    "\n",
    "        # Rotate\n",
    "\n",
    "        mat = cv2.getRotationMatrix2D((center, center), random_function() * 360, 1.0)\n",
    "        pic = cv2.warpAffine(pic, mat, (pic.shape[0], pic.shape[1]))\n",
    "\n",
    "    # Re-range\n",
    "\n",
    "    pic = (pic - np.min(pic)) / (np.max(pic) - np.min(pic) + 1e-6)\n",
    "\n",
    "    # Perspective\n",
    "\n",
    "    pic = cv2.merge((pic, pic, pic, np.ones(pic.shape))) * 255.0\n",
    "    pic, _ = rotate3d(pic, randint(-30,30), randint(-30,30), 0)\n",
    "    pic = cv2.cvtColor(pic, cv2.COLOR_BGR2GRAY) / 255.0\n",
    "\n",
    "    # Final crop\n",
    "\n",
    "    center = pic.shape[0]//2\n",
    "    pic = pic[center - height//2:center + height//2, center - width//2:center + width//2]\n",
    "\n",
    "    return pic\n",
    "\n",
    "def circular(width, height):\n",
    "\n",
    "    pic = np.zeros((height, width))\n",
    "    center = (randint(0, height),\n",
    "              randint(0, width))\n",
    "\n",
    "    diag = int((width**2 + height**2)**(1/2))\n",
    "    \n",
    "    radius = randint(diag//4, diag)\n",
    "    \n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            pic[i, j] = max(1 - (((i - center[0])**2 + (j - center[1])**2)**(1/2) / radius), 0)\n",
    "\n",
    "    pic = (pic - np.min(pic)) / (np.max(pic) - np.min(pic) + 1e-6)\n",
    "        \n",
    "    return pic\n",
    "\n",
    "def perlin(width, height, bins = 0, octaves = 4):\n",
    "\n",
    "    t_width = lcm(width, 2 ** (octaves - 1))\n",
    "    t_height = lcm(height, 2 ** (octaves - 1))\n",
    "\n",
    "    res_x = t_width//gcd(t_width, t_height)\n",
    "    res_y = t_height//gcd(t_width, t_height)\n",
    "\n",
    "    # Fractal noise\n",
    "\n",
    "    pic = generate_fractal_noise_2d((t_height, t_width), \n",
    "                                    (res_y, res_x), \n",
    "                                    octaves)\n",
    "\n",
    "    # Re-range\n",
    "\n",
    "    pic = (pic - np.min(pic)) / (np.max(pic) - np.min(pic) + 1e-6)\n",
    "\n",
    "    # Threshold\n",
    "\n",
    "    if bins > 1:\n",
    "        pic = np.digitize(pic, [(i + 1) / bins for i in range(bins - 1)]) / (bins - 1)\n",
    "    return pic\n",
    "\n",
    "def lighting_augmentation(image): \n",
    "    # check if image is 0-1 or 0-255, convert to 0-1 \n",
    "    # final image outputted is 0-255 \n",
    "    image = np.array(image, dtype=float)\n",
    "    if image.max() > 1.0:\n",
    "        # image is 0-255  \n",
    "        image = image / 255.0 \n",
    "\n",
    "    height, width = image.shape[:2] \n",
    "    \n",
    "    image_dim3 = image.shape[2] \n",
    "    augmented_image = image \n",
    "    if np.random.rand() < 0.5: \n",
    "        lines_effect = lines(width, height) \n",
    "        augmented_image *= np.repeat(lines_effect[:,:,np.newaxis],image_dim3,-1)\n",
    "    if np.random.rand() < 0.1: \n",
    "        perlin_effect = perlin(width, height) \n",
    "        augmented_image *= np.repeat(perlin_effect[:,:,np.newaxis],image_dim3,-1)\n",
    "    if np.random.rand() < 0.5: \n",
    "        gradient_effect = gradient(width, height) \n",
    "        augmented_image *= np.repeat(gradient_effect[:,:,np.newaxis],image_dim3,-1)  \n",
    "    if np.random.rand() < 0.2: \n",
    "        circular_effect = circular(width, height) \n",
    "        augmented_image *= np.repeat(circular_effect[:,:,np.newaxis],image_dim3,-1) \n",
    "    \n",
    "    # augmented_image = np.repeat(lines_effect[:,:,np.newaxis],image_dim3,-1) * np.repeat(perlin_effect[:,:,np.newaxis],image_dim3,-1) * np.repeat(gradient_effect[:,:,np.newaxis],image_dim3,-1) * np.repeat(circular_effect[:,:,np.newaxis],image_dim3,-1) * image \n",
    "    if (augmented_image.max() < 0.4) or augmented_image.max() > 1.0: # NOTE: HYPERPARAMETER \n",
    "        # renormalize image if too dark \n",
    "        pixel_max = np.random.uniform(0.9,1.0)  \n",
    "        augmented_image = pixel_max * (augmented_image - np.min(augmented_image)) / (np.max(augmented_image) - np.min(augmented_image) + 1e-6) \n",
    "    \n",
    "    augmented_image *= 255 \n",
    "    \n",
    "    return augmented_image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASS DEFINITIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datapoint:\n",
    "    def __init__(self, metadata_filepath, pose_filepath, rgb_filepath, seg_png_filepath, seg_json_filepath):\n",
    "        # Store the filepaths\n",
    "        self.metadata_filepath = metadata_filepath\n",
    "        self.pose_filepath = pose_filepath\n",
    "        self.rgb_filepath = rgb_filepath\n",
    "        self.seg_png_filepath = seg_png_filepath\n",
    "        self.seg_json_filepath = seg_json_filepath\n",
    "        \n",
    "        self.read_files()\n",
    "        self.read_pose_data() \n",
    "\n",
    "        # TODO: self.idx = get_index() # or given as input \n",
    "\n",
    "    def read_files(self): \n",
    "        # Read the actual data from files and store it\n",
    "        self.metadata = self._read_json(self.metadata_filepath) if self.metadata_filepath else None\n",
    "        self.pose = self._read_json(self.pose_filepath) if self.pose_filepath else None\n",
    "        self.rgb = self._read_rgb(self.rgb_filepath) if self.rgb_filepath else None\n",
    "        self.seg_png = self._read_segmentation_png(self.seg_png_filepath) if self.seg_png_filepath else None\n",
    "        self.seg_json = self._read_segmentation_json(self.seg_json_filepath) if self.seg_json_filepath else None \n",
    "\n",
    "    def read_pose_data(self): \n",
    "        # read pose data from pose json file \n",
    "        self.cam_pose = np.array([\n",
    "                            [1, 0, 0, 0],\n",
    "                            [0, -1, 0, 0],\n",
    "                            [0, 0, -1, 0],\n",
    "                            [0, 0, 0, 1]\n",
    "                        ]) # NOTE: cam pose from isaac sim appears to be offset \n",
    "        self.tag_pose = np.array(self.pose[\"tag\"]) \n",
    "        if self.tag_pose[0,3]==0 and self.tag_pose[1,3]==0 and self.tag_pose[2,3]==0 and self.tag_pose[3,3]==1 and self.tag_pose[3,:3].sum() != 0:  \n",
    "            self.tag_pose = self.tag_pose.transpose() \n",
    "        self.tag_pose *= np.array([\n",
    "                            [10,10,10,1],\n",
    "                            [10,10,10,1],\n",
    "                            [10,10,10,1],\n",
    "                            [1,1,1,1]\n",
    "                        ]) # rescale the tag, FIXME: avoid hardcoding tag scale value \n",
    "        self.light_pose = self.pose[\"light\"]\n",
    "        self.tag_xyzabc = np.hstack((np.array(self.tag_pose[:3,3]), R.from_matrix(self.tag_pose[:3,:3]).as_euler(\"xyz\",degrees=True))) # tag position in world frame \n",
    "\n",
    "    def compute_keypoints(self, keypoints_tag_frame, camera_matrix): \n",
    "        # transformations \n",
    "        tf_w_t = self.tag_pose \n",
    "        tf_w_c = self.cam_pose \n",
    "        tf_c_w = np.linalg.inv(tf_w_c) \n",
    "        keypoints_world_frame = [] \n",
    "        for kp_t in keypoints_tag_frame: \n",
    "            kp_t_homog = np.hstack((kp_t,np.array([1]))).reshape(4,1)\n",
    "            kp_w_homog = tf_w_t @ kp_t_homog \n",
    "            keypoints_world_frame.append(kp_w_homog[:3].reshape(3)) \n",
    "        self.keypoints_image_space = project_point_list_to_image(camera_matrix,tf_c_w,keypoints_world_frame) \n",
    "\n",
    "        return self.keypoints_image_space \n",
    "    \n",
    "    def _read_json(self, filepath):\n",
    "        \"\"\"Read and parse JSON files.\"\"\"\n",
    "        with open(filepath, 'r') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def _read_rgb(self, filepath):\n",
    "        \"\"\"Placeholder for reading RGB image files.\"\"\"\n",
    "        return filepath  # Placeholder: returning the file path to avoid memory overload\n",
    "\n",
    "    def _read_segmentation_png(self, filepath):\n",
    "        \"\"\"Placeholder for reading segmentation PNG image files.\"\"\"\n",
    "        return filepath  # Placeholder: returning the file path to avoid memory overload\n",
    "\n",
    "    def _read_segmentation_json(self, filepath):\n",
    "        \"\"\"Read segmentation JSON files.\"\"\"\n",
    "        with open(filepath, 'r') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def compute_diffusion_reflectance(self): \n",
    "        \"\"\"Compute the diffuse reflection based on pose and metadata.\"\"\"\n",
    "        N = np.array(self.tag_pose)[:3,2] \n",
    "        L = np.array(self.light_pose)[:3,2] \n",
    "        V = np.array(self.cam_pose)[:3,2] \n",
    "        light_exposure = self.metadata[\"light\"][\"exposure\"] \n",
    "        I_incident = 2**light_exposure \n",
    "        shininess = 1.0  # Placeholder value \n",
    "        self.diffuse_reflection = I_incident * max(np.dot(N, L), 0)\n",
    "\n",
    "    def preprocess_seg_img(self):\n",
    "        \"\"\"\n",
    "        Preprocesses the segmentation image by resizing and converting it to a binary mask based on tag color.\n",
    "        \"\"\"\n",
    "\n",
    "        seg_img_path = self.seg_png_filepath \n",
    "        seg_json_path = self.seg_json_filepath \n",
    "\n",
    "        # Validate that the segmentation image file exists\n",
    "        if not os.path.exists(seg_img_path):\n",
    "            raise FileNotFoundError(f\"Segmentation image file not found: {seg_img_path}\")\n",
    "\n",
    "        # Validate that the JSON file exists\n",
    "        if not os.path.exists(seg_json_path):\n",
    "            raise FileNotFoundError(f\"Segmentation JSON file not found: {seg_json_path}\")\n",
    "\n",
    "        # Load the segmentation JSON data \n",
    "        with open(seg_json_path, 'r') as json_file:\n",
    "            seg_json = json.load(json_file)\n",
    "\n",
    "            # Find the tag color from the JSON data\n",
    "            for key, val in seg_json.items(): \n",
    "                if val.get(\"class\") == \"tag0\":  \n",
    "                    # Convert the key (which is a string representing a tuple) into an actual tuple\n",
    "                    tag_seg_color = tuple(map(int, key.strip('()').split(', ')))  # Convert string '(140, 25, 255, 255)' into a tuple (140, 25, 255, 255)\n",
    "                    break\n",
    "            else:\n",
    "                # raise ValueError(\"Tag with class 'tag0' not found in JSON.\")\n",
    "                tag_seg_color = tuple([-1,-1,-1,-1]) # impossible color value # FIXME: this is a workaround which can be turned into something more elegant \n",
    "\n",
    "        # Load and resize the segmentation image\n",
    "        seg_img = Image.open(seg_img_path)\n",
    "        # new_size = (480, 270)\n",
    "        # new_size = (480*2, 270*2)\n",
    "        # seg_img_resized = seg_img.resize(new_size)\n",
    "        seg_img_resized = seg_img\n",
    "\n",
    "        # Convert the resized image to a NumPy array\n",
    "        seg_img_resized = np.array(seg_img_resized)\n",
    "\n",
    "        # Check if the image is RGB (3 channels) or RGBA (4 channels) or grayscale (1 channel)\n",
    "        if len(seg_img_resized.shape) == 3:\n",
    "            if seg_img_resized.shape[2] == 3:  # RGB image\n",
    "                # Compare each pixel to the tag color (e.g., RGB triplet)\n",
    "                seg_img_resized = np.all(seg_img_resized == tag_seg_color[:3], axis=-1)  # Create binary mask for RGB image\n",
    "            elif seg_img_resized.shape[2] == 4:  # RGBA image\n",
    "                # Compare each pixel to the tag color (RGBA)\n",
    "                seg_img_resized = np.all(seg_img_resized == tag_seg_color, axis=-1)  # Create binary mask for RGBA image\n",
    "        else:  # If it's a single channel (grayscale), use it directly\n",
    "            seg_img_resized = seg_img_resized == tag_seg_color  # Compare pixel values directly\n",
    "\n",
    "        # Convert the binary mask to uint8 type (0 or 1)\n",
    "        seg_img_resized = (seg_img_resized).astype(np.uint8) * 255  # Multiply by 255 to match image range\n",
    "\n",
    "        # Convert the binary mask back to an image\n",
    "        seg_img_resized = Image.fromarray(seg_img_resized)\n",
    "\n",
    "        return seg_img_resized\n",
    "    \n",
    "    def get_roi_image(self, seg=None, roi_size=128, padding=5): \n",
    "        if seg is None: \n",
    "            seg = self.preprocess_seg_img() \n",
    "\n",
    "        image_border_size = np.max([np.array(seg).shape[0], np.array(seg).shape[1]]) \n",
    "\n",
    "        # get pixel info of seg \n",
    "        seg = np.array(seg) \n",
    "        seg = cv2.copyMakeBorder(seg, image_border_size, image_border_size, image_border_size, image_border_size, cv2.BORDER_CONSTANT, value=0) \n",
    "        tag_pixels = np.argwhere(seg == 255)\n",
    "        seg_tag_min_x = np.min(tag_pixels[:,1])\n",
    "        seg_tag_max_x = np.max(tag_pixels[:,1])\n",
    "        seg_tag_min_y = np.min(tag_pixels[:,0])\n",
    "        seg_tag_max_y = np.max(tag_pixels[:,0])\n",
    "        seg_height = seg_tag_max_y - seg_tag_min_y  \n",
    "        seg_width = seg_tag_max_x - seg_tag_min_x \n",
    "        seg_center_x = (seg_tag_min_x + seg_tag_max_x) // 2\n",
    "        seg_center_y = (seg_tag_min_y + seg_tag_max_y) // 2 \n",
    "\n",
    "        # get pixel info of rgb \n",
    "        rgb = np.array(Image.open(self.rgb_filepath))\n",
    "        rgb = cv2.copyMakeBorder(rgb, image_border_size, image_border_size, image_border_size, image_border_size, cv2.BORDER_CONSTANT, value=0) \n",
    "        rgb_side = max(seg_height, seg_width) + 2*padding \n",
    "        rgb_tag_min_x = seg_center_x - rgb_side // 2\n",
    "        rgb_tag_max_x = seg_center_x + rgb_side // 2\n",
    "        rgb_tag_min_y = seg_center_y - rgb_side // 2\n",
    "        rgb_tag_max_y = seg_center_y + rgb_side // 2\n",
    "        roi_img = rgb[rgb_tag_min_y:rgb_tag_max_y, rgb_tag_min_x:rgb_tag_max_x, :]\n",
    "\n",
    "        # resize rgb bbox to roi size         \n",
    "        try: \n",
    "            self.roi_img = cv2.resize(roi_img, (roi_size, roi_size))\n",
    "        except: \n",
    "            print(\"error resizing\") \n",
    "            import pdb; pdb.set_trace() \n",
    "\n",
    "        W = rgb.shape[1] \n",
    "        H = rgb.shape[0]\n",
    "        self.roi_coordinates = np.array([rgb_tag_min_x-W/2, rgb_tag_max_x-W/2, rgb_tag_min_y-H/2, rgb_tag_max_y-H/2]) # FIXME: there is some issue here # image (x,y) coordinates (origin at image center) \n",
    "\n",
    "        self.roi_center = np.array([seg_center_x, seg_center_y]) - np.array([image_border_size, image_border_size]) \n",
    "\n",
    "        self.W_img = W \n",
    "        self.H_img = H \n",
    "        self.img_center = np.array([W/2,H/2])\n",
    "\n",
    "        return self.roi_img, self.roi_coordinates, self.roi_center \n",
    "    \n",
    "    def get_roi_keypoints(self): \n",
    "\n",
    "        # check if keypoints and roi have been computed, else return None \n",
    "        if not hasattr(self, 'keypoints_image_space') or not hasattr(self, 'roi_img'): \n",
    "            return None \n",
    "        \n",
    "        # get keypoints in roi space\n",
    "        roi_keypoints = []\n",
    "        for kp in self.keypoints_image_space: \n",
    "            s = np.array(self.roi_img.shape[:2]) \n",
    "            w = self.roi_coordinates[1] - self.roi_coordinates[0]\n",
    "            h = self.roi_coordinates[3] - self.roi_coordinates[2]\n",
    "            m = s / np.array([w, h])     \n",
    "            # m = s / np.array([self.W_img, self.H_img])     \n",
    "            kp_roi = m*(kp - self.roi_center) + s/2 \n",
    "            roi_keypoints.append(kp_roi) \n",
    "\n",
    "        self.roi_keypoints = roi_keypoints \n",
    "\n",
    "        return self.roi_keypoints  \n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Custom representation for the datapoint object.\"\"\"\n",
    "        # return f\"datapoint(metadata_filepath={self.metadata_filepath}, pose_filepath={self.pose_filepath}, rgb_filepath={self.rgb_filepath}, seg_png_filepath={self.seg_png_filepath}, seg_json_filepath={self.seg_json_filepath})\"\n",
    "        description = [\n",
    "            f\"lighting_exposure={self.metadata[\"light\"][\"exposure\"]:.2f}\",\n",
    "            # f\"lighting_color={str(self.metadata[\"light\"][\"color\"]) }\" # FIXME: reduce to two decimal places \n",
    "            f\"lighting_color=({self.metadata[\"light\"][\"color\"][0]:.2f},{self.metadata[\"light\"][\"color\"][1]:.2f},{self.metadata[\"light\"][\"color\"][2]:.2f})\" # FIXME: reduce to two decimal places \n",
    "        ]\n",
    "        return \"\\n\".join(description) \n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, data_folders, out_dir):\n",
    "        self.data_folders = data_folders\n",
    "        self.out_dir = out_dir\n",
    "        self.datapoints = []\n",
    "        self.datapoints_train = []\n",
    "        self.datapoints_val = []\n",
    "\n",
    "        self.set_augmentation_transforms() \n",
    "        self.set_camera(camera_name=\"isaac\") \n",
    "\n",
    "    def _get_files_in_subfolder(self, folder, file_extension=None):\n",
    "        \"\"\"Helper method to get files in a subfolder, with an optional file extension filter.\"\"\"\n",
    "        files_list = os.listdir(folder)\n",
    "        if file_extension:\n",
    "            files_list = [file for file in files_list if file.endswith(file_extension)]\n",
    "        # Order files_list by date created\n",
    "        files_list = sorted(files_list, key=lambda x: os.path.getctime(os.path.join(folder, x)))  # Assumes creation dates are synchronized\n",
    "        return files_list\n",
    "    \n",
    "    def set_marker(self, image_path, num_squares, side_length): \n",
    "        self.marker_path = image_path \n",
    "        self.marker_image = Image.open(image_path) \n",
    "        self.marker_num_squares = num_squares \n",
    "        self.marker_side_length = side_length \n",
    "        self.keypoints_tag_frame = compute_2D_gridpoints(N=self.marker_num_squares, s=self.marker_side_length) \n",
    "\n",
    "    def set_camera(self, camera_name=\"isaac\", camera_matrix=None):  \n",
    "        # default camera is isaac \n",
    "        if camera_name == \"isaac\": \n",
    "            # camera parameters \n",
    "            width = 640 \n",
    "            height = 480 \n",
    "            focal_length = 24.0 \n",
    "            horiz_aperture = 20.955\n",
    "            # Pixels are square so we can do:\n",
    "            vert_aperture = height/width * horiz_aperture\n",
    "            fov = 2 * math.atan(horiz_aperture / (2 * focal_length))\n",
    "            # compute focal point and center\n",
    "            fx = width * focal_length / horiz_aperture\n",
    "            fy = height * focal_length / vert_aperture\n",
    "            cx = width / 2\n",
    "            cy = height /2 \n",
    "\n",
    "            self.camera_matrix = np.array([\n",
    "                [fx,0,cx],\n",
    "                [0,fy,cy],\n",
    "                [0,0,1]\n",
    "            ])\n",
    "        if camera_matrix is not None: \n",
    "            self.camera_matrix = camera_matrix \n",
    "\n",
    "    def process_folders(self):\n",
    "        \"\"\"Process the folders and create datapoint objects.\"\"\"\n",
    "        for data_folder in self.data_folders:\n",
    "            metadata_subfolder = os.path.join(data_folder, \"metadata\")\n",
    "            pose_subfolder = os.path.join(data_folder, \"pose\")\n",
    "            rgb_subfolder = os.path.join(data_folder, \"rgb\")\n",
    "            seg_subfolder = os.path.join(data_folder, \"seg\")\n",
    "\n",
    "            # List files in subfolders \n",
    "            metadata_files = self._get_files_in_subfolder(metadata_subfolder, file_extension=\".json\")\n",
    "            pose_files = self._get_files_in_subfolder(pose_subfolder, file_extension=\".json\")\n",
    "            rgb_files = self._get_files_in_subfolder(rgb_subfolder, file_extension=\".png\")\n",
    "            seg_png_files = self._get_files_in_subfolder(seg_subfolder, file_extension=\".png\")\n",
    "            seg_json_files = self._get_files_in_subfolder(seg_subfolder, file_extension=\".json\")\n",
    "\n",
    "            # Make sure the files are indexed and aligned properly (by index) across the subfolders\n",
    "            max_length = max(len(metadata_files), len(pose_files), len(rgb_files), len(seg_png_files), len(seg_json_files))\n",
    "\n",
    "            # Verify that the lengths are the same\n",
    "            if not all(len(files) == max_length for files in [metadata_files, pose_files, rgb_files, seg_png_files, seg_json_files]):\n",
    "                print(f\"Lengths do not match for folder: {data_folder}\")\n",
    "                continue\n",
    "\n",
    "            for i in range(max_length):\n",
    "                # Use index 'i' to fetch corresponding files. If a file doesn't exist, use None.\n",
    "                metadata_filepath = os.path.join(metadata_subfolder, metadata_files[i]) if i < len(metadata_files) else None\n",
    "                pose_filepath = os.path.join(pose_subfolder, pose_files[i]) if i < len(pose_files) else None\n",
    "                rgb_filepath = os.path.join(rgb_subfolder, rgb_files[i]) if i < len(rgb_files) else None\n",
    "                seg_png_filepath = os.path.join(seg_subfolder, seg_png_files[i]) if i < len(seg_png_files) else None\n",
    "                seg_json_filepath = os.path.join(seg_subfolder, seg_json_files[i]) if i < len(seg_json_files) else None\n",
    "\n",
    "                # Create a datapoint object for each corresponding file\n",
    "                data_point = datapoint(metadata_filepath, pose_filepath, rgb_filepath, seg_png_filepath, seg_json_filepath)\n",
    "                self.datapoints.append(data_point)\n",
    "\n",
    "    def get_datapoints(self):\n",
    "        \"\"\"Return the list of datapoint objects.\"\"\"\n",
    "        return self.datapoints\n",
    "    \n",
    "    def get_datapoints_filtered(self):\n",
    "        \"\"\"Return the list of filtered datapoint objects.\"\"\"\n",
    "        return self.datapoints_filtered \n",
    "    \n",
    "    def check_image_okay(self, rgb_img, seg_img, min_tag_area=1000, min_tag_pix_mean=25, max_tag_pix_mean=250): \n",
    "        if rgb_img is None: \n",
    "            return False \n",
    "        seg_img = np.array(seg_img) \n",
    "        # compute pixel area of tag segmentation \n",
    "        tag_pix_area = np.sum(seg_img == 255) \n",
    "\n",
    "        # create list of marker pixels using segmentation \n",
    "        marker_pixels = np.argwhere(seg_img == 255)  # Get the indices of pixels where the tag is present \n",
    "        # compute contrast of marker pixels using rgb image \n",
    "        rgb_img = np.array(rgb_img) \n",
    "        if rgb_img.max() <= 1.0: \n",
    "            rgb_img *= 255.0 \n",
    "        marker_rgb_values = rgb_img[marker_pixels[:, 0], marker_pixels[:, 1]]  # Get the RGB values of the marker pixels \n",
    "        marker_grey_values = np.mean(marker_rgb_values, axis=1)  # Compute the mean RGB values of the marker pixels \n",
    "        # compute contrast as the difference in magnitude of the RGB values of the marker pixels \n",
    "        tag_pix_contrast = marker_grey_values.max() - marker_grey_values.min()  \n",
    "        tag_pix_mean = marker_grey_values.mean()\n",
    "        if tag_pix_area > min_tag_area and tag_pix_mean > min_tag_pix_mean and tag_pix_mean < max_tag_pix_mean:  # FIXME: hardcoded threshold for tag area and diffuse reflection \n",
    "            bool_image_ok = True \n",
    "        else:\n",
    "            bool_image_ok = False\n",
    "            # if not (tag_pix_mean > min_tag_pix_mean): \n",
    "            #     print(\"tag pix mean too low \") \n",
    "            # if not (tag_pix_mean < max_tag_pix_mean):\n",
    "            #     print(\"tag pix mean too high\")      \n",
    "        return bool_image_ok\n",
    "\n",
    "    def filter_datapoints(self, min_tag_area=1000, min_tag_pix_mean=70, max_tag_pix_mean=250): \n",
    "        \"\"\"Compute the diffusion reflectance and only keep datapoints with positive values.\"\"\"\n",
    "        self.datapoints_filtered = [] \n",
    "        self.datapoints_filtered_out = [] \n",
    "        for idx, dp in enumerate(self.datapoints):\n",
    "            dp.compute_diffusion_reflectance() \n",
    "            seg_img = dp.preprocess_seg_img() \n",
    "            seg_img = np.array(seg_img) \n",
    "            # compute pixel area of tag segmentation \n",
    "            dp.tag_pix_area = np.sum(seg_img == 255) \n",
    "            self.datapoints[idx].tag_pix_area = np.sum(seg_img == 255) \n",
    "\n",
    "            # create list of marker pixels using segmentation \n",
    "            marker_pixels = np.argwhere(seg_img == 255)  # Get the indices of pixels where the tag is present \n",
    "            # compute contrast of marker pixels using rgb image \n",
    "            rgb_img = Image.open(dp.rgb_filepath) \n",
    "            rgb_img = np.array(rgb_img) \n",
    "            marker_rgb_values = rgb_img[marker_pixels[:, 0], marker_pixels[:, 1]]  # Get the RGB values of the marker pixels \n",
    "            marker_grey_values = np.mean(marker_rgb_values, axis=1)  # Compute the mean RGB values of the marker pixels \n",
    "            # compute contrast as the difference in magnitude of the RGB values of the marker pixels \n",
    "\n",
    "            if marker_grey_values.size > 0: \n",
    "                pass \n",
    "            else: \n",
    "                print(f\"empty image at {dp.rgb_filepath}\")\n",
    "                self.datapoints_filtered_out.append(dp) \n",
    "                continue \n",
    "\n",
    "            dp.tag_pix_contrast = marker_grey_values.max() - marker_grey_values.min()  \n",
    "            self.datapoints[idx].tag_pix_contrast = marker_grey_values.max() - marker_grey_values.min() \n",
    "            self.datapoints[idx].tag_pix_mean = marker_grey_values.mean()\n",
    "            dp.tag_pix_mean = marker_grey_values.mean() \n",
    "\n",
    "            # os.makedirs(os.path.join(self.out_dir, \"contrast\"), exist_ok=True) \n",
    "            # # save contrast image for debugging purposes\n",
    "            # cv2.imwrite(os.path.join(self.out_dir, \"contrast\", f\"contrast_{idx}_{dp.tag_pix_contrast}_{dp.tag_pix_mean}.png\"), rgb_img) \n",
    "            if self.check_image_okay(rgb_img, seg_img, min_tag_area=min_tag_area, min_tag_pix_mean=min_tag_pix_mean, max_tag_pix_mean=max_tag_pix_mean): \n",
    "                self.datapoints_filtered.append(dp)\n",
    "            else: \n",
    "                self.datapoints_filtered_out.append(dp)\n",
    "\n",
    "            # if dp.diffuse_reflection > min_diffuse_reflection and dp.tag_pix_area > min_tag_area and dp.tag_pix_mean > min_tag_pix_mean and dp.tag_pix_mean < max_tax_pix_mean:  # FIXME: hardcoded threshold for tag area and diffuse reflection \n",
    "            #     self.datapoints_filtered.append(dp) \n",
    "            # else: \n",
    "            #     self.datapoints_filtered_out.append(dp) \n",
    "\n",
    "            if idx % (len(self.datapoints)/10) == 0: \n",
    "                print(f\"Processed {idx} / {len(self.datapoints)}\") \n",
    "                \n",
    "    def split_train_val_data(self, filter=True, frac_train=0.8, num_points_max=-1):\n",
    "        \"\"\"Split the datapoints into training and validation datasets.\"\"\"\n",
    "        if num_points_max == -1: \n",
    "            if filter: \n",
    "                num_points = len(self.datapoints_filtered) \n",
    "            else: \n",
    "                num_points = len(self.datapoints)\n",
    "        else: \n",
    "            if filter: \n",
    "                num_points = np.min([num_points_max, len(self.datapoints_filtered)])\n",
    "            else: \n",
    "                num_points = np.min([num_points_max, len(self.datapoints)])\n",
    "\n",
    "        if filter: \n",
    "            self.datapoints_train = random.sample(self.datapoints_filtered, int(frac_train * num_points))\n",
    "            non_training_datapoints = [dp for dp in self.datapoints_filtered if dp not in self.datapoints_train]\n",
    "            self.datapoints_val = random.sample(non_training_datapoints, int((1-frac_train) * num_points)) \n",
    "        else:\n",
    "            self.datapoints_train = random.sample(self.datapoints, int(frac_train * num_points)) \n",
    "            non_training_datapoints = [dp for dp in self.datapoints if dp not in self.datapoints_train]\n",
    "            self.datapoints_val = random.sample(non_training_datapoints, int((1-frac_train) * num_points)) \n",
    "\n",
    "    def create_directories(self):\n",
    "        \"\"\"Create directories for training and validation data.\"\"\"\n",
    "        dir_train = os.path.join(self.out_dir, \"train\")\n",
    "        dir_val = os.path.join(self.out_dir, \"val\")\n",
    "        dir_train_rgb = os.path.join(dir_train, \"rgb\")\n",
    "        dir_train_seg = os.path.join(dir_train, \"seg\")\n",
    "        dir_val_rgb = os.path.join(dir_val, \"rgb\")\n",
    "        dir_val_seg = os.path.join(dir_val, \"seg\")\n",
    "\n",
    "        os.makedirs(dir_train_rgb, exist_ok=True)\n",
    "        os.makedirs(dir_train_seg, exist_ok=True)\n",
    "        os.makedirs(dir_val_rgb, exist_ok=True)\n",
    "        os.makedirs(dir_val_seg, exist_ok=True)\n",
    "\n",
    "        return dir_train_rgb, dir_train_seg, dir_val_rgb, dir_val_seg\n",
    "\n",
    "    def preprocess_rgb(self, img_path):  \n",
    "        \"\"\"Preprocess RGB image by resizing it.\"\"\"\n",
    "        # new_size = (480, 270)  # Define the new size\n",
    "        # new_size = (480*2, 270*2)  # Define the new size\n",
    "        img = Image.open(img_path)\n",
    "        # img_resized = img.resize(new_size)\n",
    "        img_resized = img \n",
    "        return img_resized\n",
    "\n",
    "    def preprocess_seg_img(self, seg_img_path, seg_json_path, tag_seg_color=None):\n",
    "        \"\"\"\n",
    "        Preprocesses the segmentation image by resizing and converting it to a binary mask based on tag color.\n",
    "        \"\"\"\n",
    "        # Validate that the segmentation image file exists\n",
    "        if not os.path.exists(seg_img_path):\n",
    "            raise FileNotFoundError(f\"Segmentation image file not found: {seg_img_path}\")\n",
    "\n",
    "        # Validate that the JSON file exists\n",
    "        if not os.path.exists(seg_json_path):\n",
    "            raise FileNotFoundError(f\"Segmentation JSON file not found: {seg_json_path}\")\n",
    "\n",
    "        # Load the segmentation JSON data if tag_seg_color is not provided\n",
    "        if tag_seg_color is None:\n",
    "            with open(seg_json_path, 'r') as json_file:\n",
    "                seg_json = json.load(json_file)\n",
    "\n",
    "            # Find the tag color from the JSON data\n",
    "            for key, val in seg_json.items(): \n",
    "                if val.get(\"class\") == \"tag0\":  \n",
    "                    # Convert the key (which is a string representing a tuple) into an actual tuple\n",
    "                    tag_seg_color = tuple(map(int, key.strip('()').split(', ')))  # Convert string '(140, 25, 255, 255)' into a tuple (140, 25, 255, 255)\n",
    "                    break\n",
    "            else:\n",
    "                # raise ValueError(\"Tag with class 'tag0' not found in JSON.\")\n",
    "                tag_seg_color = tuple([-1,-1,-1,-1]) # impossible color value # FIXME: this is a workaround which can be turned into something more elegant \n",
    "\n",
    "        # Load and resize the segmentation image\n",
    "        seg_img = Image.open(seg_img_path)\n",
    "        # new_size = (480, 270)\n",
    "        # new_size = (480*2, 270*2)\n",
    "        # seg_img_resized = seg_img.resize(new_size)\n",
    "        seg_img_resized = seg_img\n",
    "\n",
    "        # Convert the resized image to a NumPy array\n",
    "        seg_img_resized = np.array(seg_img_resized)\n",
    "\n",
    "        # Check if the image is RGB (3 channels) or RGBA (4 channels) or grayscale (1 channel)\n",
    "        if len(seg_img_resized.shape) == 3:\n",
    "            if seg_img_resized.shape[2] == 3:  # RGB image\n",
    "                # Compare each pixel to the tag color (e.g., RGB triplet)\n",
    "                seg_img_resized = np.all(seg_img_resized == tag_seg_color[:3], axis=-1)  # Create binary mask for RGB image\n",
    "            elif seg_img_resized.shape[2] == 4:  # RGBA image\n",
    "                # Compare each pixel to the tag color (RGBA)\n",
    "                seg_img_resized = np.all(seg_img_resized == tag_seg_color, axis=-1)  # Create binary mask for RGBA image\n",
    "        else:  # If it's a single channel (grayscale), use it directly\n",
    "            seg_img_resized = seg_img_resized == tag_seg_color  # Compare pixel values directly\n",
    "\n",
    "        # Convert the binary mask to uint8 type (0 or 1)\n",
    "        seg_img_resized = (seg_img_resized).astype(np.uint8) * 255  # Multiply by 255 to match image range\n",
    "\n",
    "        # Convert the binary mask back to an image\n",
    "        seg_img_resized = Image.fromarray(seg_img_resized)\n",
    "\n",
    "        return seg_img_resized\n",
    "\n",
    "    def save_preprocessed_images(self, frac_train=0.8, augmentation=True, n_augmentations=0):\n",
    "        \"\"\"Loop through train and val datapoints and save preprocessed images and segmentation masks.\"\"\"\n",
    "        dir_train_rgb, dir_train_seg, dir_val_rgb, dir_val_seg = self.create_directories()\n",
    "\n",
    "        if augmentation: \n",
    "            transform = A.Compose([\n",
    "                # A.RandomShadow(shadow_roi=(0,0,1,1), num_shadows_limit=(1,10), shadow_dimension=4, shadow_intensity_range =(0.5, 0.8), p=0.8),  # Apply random shadows to the image\n",
    "                A.RandomSunFlare(flare_roi=(0,0,1,1), num_flare_circles_range=(10,50), src_radius=100, src_color=(150,150,150), method=\"physics_based\", p=0.8),  # Apply random sun flare to the image, TODO: come back to this, get labels \n",
    "                A.GaussNoise(var_limit=(0,0.01), per_channel=True, p=1),  # Add noise to the image \n",
    "                A.AdvancedBlur(blur_limit=(5,25), p=0.8),  # Apply blur to the image \n",
    "                # A.RandomGamma(gamma_limit=(80, 120), p=0.8),  # Apply gamma correction to the image\n",
    "                # A.RandomBrightnessContrast(brightness_limit=(-0.25,0.25), contrast_limit=(-0.95,0.95), p=0.8),  # Adjust brightness and contrast\n",
    "                # A.ISONoise(intensity=(0.1, 0.5), color_shift=(0.01, 0.05), p=0.8),  # Apply ISO noise to the image \n",
    "            ]) \n",
    "\n",
    "        for i, dp in enumerate(self.datapoints_train): \n",
    "            img = self.preprocess_rgb(dp.rgb_filepath) \n",
    "            seg = self.preprocess_seg_img(dp.seg_png_filepath, dp.seg_json_filepath)   \n",
    "            \n",
    "            img.save(os.path.join(dir_train_rgb, f\"img_{i+1}_0.png\")) \n",
    "            seg.save(os.path.join(dir_train_seg, f\"seg_{i+1}.png\"))\n",
    "            \n",
    "            if augmentation: \n",
    "                for j in range(n_augmentations): \n",
    "                    augmented_img = transform(image=np.array(img)[:,:,:3])['image']\n",
    "                    augmented_img = lighting_augmentation(augmented_img) \n",
    "                    retry = 0 \n",
    "                    while not self.check_image_okay(augmented_img, seg, min_tag_area=1000, min_tag_pix_mean=25, max_tag_pix_mean=250): \n",
    "                        # print(f\"Retry: {retry}\")\n",
    "                        augmented_img = transform(image=np.array(img)[:,:,:3])['image']\n",
    "                        if retry < 5: \n",
    "                            augmented_img = lighting_augmentation(augmented_img) \n",
    "                        if retry > 25: \n",
    "                            print(f\"Exceeded retry limit. Skipping image.\")\n",
    "                            continue \n",
    "                        retry += 1\n",
    "                    # augmented_img.save(os.path.join(dir_train_rgb, f\"img_{i}_{j}.png\")) \n",
    "                    image = Image.fromarray((augmented_img).astype(np.uint8)) \n",
    "                    image.save(os.path.join(dir_train_rgb, f\"img_{i}_{j}.png\")) \n",
    "\n",
    "            # print progress \n",
    "            if i % (len(self.datapoints_train)/100) == 0: \n",
    "                print(f\"Processed training data: {i}/{len(self.datapoints_train)}\")\n",
    "\n",
    "        for i, dp in enumerate(self.datapoints_val):\n",
    "            img = self.preprocess_rgb(dp.rgb_filepath) \n",
    "            seg = self.preprocess_seg_img(dp.seg_png_filepath, dp.seg_json_filepath) \n",
    "\n",
    "            img.save(os.path.join(dir_val_rgb, f\"img_{i+1}_0.png\")) \n",
    "            seg.save(os.path.join(dir_val_seg, f\"seg_{i+1}.png\"))\n",
    "\n",
    "            if augmentation: \n",
    "                for j in range(n_augmentations): \n",
    "                    augmented_img = transform(image=np.array(img)[:,:,:3])['image']\n",
    "                    augmented_img = lighting_augmentation(augmented_img) \n",
    "                    retry = 0 \n",
    "                    while not self.check_image_okay(augmented_img, seg, min_tag_area=1000, min_tag_pix_mean=25, max_tag_pix_mean=250): \n",
    "                        augmented_img = transform(image=np.array(img)[:,:,:3])['image']\n",
    "                        if retry < 5: \n",
    "                            augmented_img = lighting_augmentation(augmented_img) \n",
    "                        if retry > 25:\n",
    "                            print(f\"Exceeded retry limit. Skipping image.\")\n",
    "                            continue  \n",
    "                        retry += 1 \n",
    "\n",
    "                    # augmented_img.save(os.path.join(dir_val_rgb, f\"img_{i}_{j}.png\")) \n",
    "                    image = Image.fromarray((augmented_img * 255).astype(np.uint8)) \n",
    "                    image.save(os.path.join(dir_val_rgb, f\"img_{i}_{j}.png\")) \n",
    "            # print progress \n",
    "            if i % (len(self.datapoints_val)/100) == 0: \n",
    "                print(f\"Processed training data: {i}/{len(self.datapoints_val)}\")\n",
    "\n",
    "    def set_augmentation_transforms(self): \n",
    "        transform = A.Compose([\n",
    "                # A.RandomShadow(shadow_roi=(0,0,1,1), num_shadows_limit=(1,10), shadow_dimension=4, shadow_intensity_range =(0.5, 0.8), p=0.8),  # Apply random shadows to the image\n",
    "                A.RandomSunFlare(flare_roi=(0,0,1,1), num_flare_circles_range=(10,50), src_radius=100, src_color=(150,150,150), method=\"physics_based\", p=0.8),  # Apply random sun flare to the image, TODO: come back to this, get labels \n",
    "                A.GaussNoise(var_limit=(0,0.001), per_channel=True, p=1),  # Add noise to the image \n",
    "                A.AdvancedBlur(blur_limit=(5,25), p=0.8),  # Apply blur to the image \n",
    "                # A.RandomGamma(gamma_limit=(80, 120), p=0.8),  # Apply gamma correction to the image\n",
    "                # A.RandomBrightnessContrast(brightness_limit=(-0.25,0.25), contrast_limit=(-0.95,0.95), p=0.8),  # Adjust brightness and contrast\n",
    "                # A.ISONoise(intensity=(0.1, 0.5), color_shift=(0.01, 0.05), p=0.8),  # Apply ISO noise to the image \n",
    "            ])\n",
    "        self.albumentations_transform = transform \n",
    "\n",
    "    def augment_image(self, image, seg, max_attempts_lighting=5, max_attempts_combined=10):  \n",
    "        image = np.array(image)[:,:,:3]  \n",
    "        # check if image is okay \n",
    "        if not self.check_image_okay(image, seg, min_tag_area=1000, min_tag_pix_mean=50, max_tag_pix_mean=250): \n",
    "            return None  \n",
    "        # apply albumentations augmentation \n",
    "        augmented_image = None \n",
    "        for attempt in range(max_attempts_combined): \n",
    "            # apply albumentations augmentation \n",
    "            augmented_image = self.albumentations_transform(image=image)[\"image\"] \n",
    "            # apply lighting augmentation\n",
    "            if attempt < max_attempts_lighting: \n",
    "                augmented_image = lighting_augmentation(augmented_image) \n",
    "            if not self.check_image_okay(augmented_image, seg): \n",
    "                # print(f\"Augmentation attempt {attempt} failed, brightening image.\") \n",
    "                image = cv2.convertScaleAbs(image, alpha=1, beta=25) \n",
    "                continue \n",
    "            else: \n",
    "                break\n",
    "        if attempt == max_attempts_combined: \n",
    "            print(\"Failed to augment image after max attempts.\")\n",
    "            return None\n",
    "        return augmented_image \n",
    "    \n",
    "    def save_train_val_data(self, \n",
    "                            save_rgb=True, \n",
    "                            save_seg=True, \n",
    "                            save_keypoints=True, \n",
    "                            save_metadata=True, \n",
    "                            num_augmentations=0,\n",
    "                            save_summary_image=False,\n",
    "                            save_roi=True, \n",
    "                            ):\n",
    "        \n",
    "        # Create directories for both training and validation datasets\n",
    "        self.train_dir = os.path.join(self.out_dir, \"train\")\n",
    "        self.val_dir = os.path.join(self.out_dir, \"val\")\n",
    "        os.makedirs(self.train_dir, exist_ok=True)\n",
    "        os.makedirs(self.val_dir, exist_ok=True)\n",
    "\n",
    "        # Loop over train and val\n",
    "        for dataset_type in ['train', 'val']:\n",
    "            dataset_dir = self.train_dir if dataset_type == 'train' else self.val_dir\n",
    "            datapoints = self.datapoints_train if dataset_type == 'train' else self.datapoints_val\n",
    "\n",
    "            # Create specific directories for RGB, Segmentation, Keypoints, Metadata, and Summary Images\n",
    "            if save_rgb: \n",
    "                os.makedirs(os.path.join(dataset_dir, \"rgb\"), exist_ok=True)\n",
    "            if save_seg:\n",
    "                os.makedirs(os.path.join(dataset_dir, \"seg\"), exist_ok=True)\n",
    "            if save_keypoints:\n",
    "                os.makedirs(os.path.join(dataset_dir, \"keypoints\"), exist_ok=True)\n",
    "            if save_metadata:\n",
    "                os.makedirs(os.path.join(dataset_dir, \"metadata\"), exist_ok=True)\n",
    "            if save_summary_image:\n",
    "                os.makedirs(os.path.join(dataset_dir, \"summary_images\"), exist_ok=True)\n",
    "            if save_roi:\n",
    "                os.makedirs(os.path.join(dataset_dir, \"roi_rgb\"), exist_ok=True)\n",
    "                os.makedirs(os.path.join(dataset_dir, \"roi_keypoints\"), exist_ok=True) \n",
    "\n",
    "            # Process each datapoint in the current dataset\n",
    "            for i, dp in enumerate(datapoints):\n",
    "                if save_rgb:\n",
    "                    rgb_img = Image.open(dp.rgb_filepath)\n",
    "                    if num_augmentations == 0:\n",
    "                        rgb_img.save(os.path.join(dataset_dir, \"rgb\", f\"img_{i}.png\"))\n",
    "                    else:\n",
    "                        # augment image \n",
    "                        for j in range(num_augmentations):\n",
    "                            seg_img = dp.preprocess_seg_img()\n",
    "                            augmented_img = self.augment_image(rgb_img, seg_img)\n",
    "                            if augmented_img is None:\n",
    "                                # print(f\"Failed to augment image {i}.\")\n",
    "                                continue\n",
    "                            else:\n",
    "                                augmented_img = Image.fromarray((augmented_img).astype(np.uint8))\n",
    "                                augmented_img.save(os.path.join(dataset_dir, \"rgb\", f\"img_{i}_{j}.png\"))\n",
    "\n",
    "                if save_seg:\n",
    "                    seg_img = dp.preprocess_seg_img()\n",
    "                    seg_img.save(os.path.join(dataset_dir, \"seg\", f\"seg_{i}.png\"))\n",
    "\n",
    "                if save_keypoints:\n",
    "                    keypoints = dp.compute_keypoints(self.keypoints_tag_frame, self.camera_matrix)\n",
    "                    keypoints_json = {}\n",
    "                    for i_kp, kp in enumerate(keypoints):\n",
    "                        keypoints_json[f\"keypoints_{i_kp}\"] = kp.tolist()\n",
    "                    with open(os.path.join(dataset_dir, \"keypoints\", f\"keypoints_{i}.json\"), 'w') as f:\n",
    "                        json.dump(keypoints_json, f)\n",
    "\n",
    "                if save_metadata:\n",
    "                    metadata = dp.metadata\n",
    "                    with open(os.path.join(dataset_dir, \"metadata\", f\"metadata_{i}.json\"), 'w') as f:\n",
    "                        json.dump(metadata, f)\n",
    "\n",
    "                if save_roi:\n",
    "                    roi_image, roi_coordinates, roi_center = dp.get_roi_image(seg=seg_img)\n",
    "                    roi_image = Image.fromarray(roi_image)\n",
    "                    roi_image.save(os.path.join(dataset_dir, \"roi_rgb\", f\"roi_{i}.png\")) \n",
    "                    roi_keypoints = dp.get_roi_keypoints()\n",
    "                    if roi_keypoints is not None:\n",
    "                        roi_keypoints_json = {}\n",
    "                        for i_kp, kp in enumerate(roi_keypoints):\n",
    "                            roi_keypoints_json[f\"keypoints_{i_kp}\"] = kp.tolist()\n",
    "                        with open(os.path.join(dataset_dir, \"roi_keypoints\", f\"roi_keypoints_{i}.json\"), 'w') as f:\n",
    "                            json.dump(roi_keypoints_json, f)\n",
    "\n",
    "                if save_summary_image:\n",
    "                    # Check if images are loaded correctly\n",
    "                    if rgb_img is None:\n",
    "                        raise ValueError(f\"RGB image at {dp.rgb_filepath} could not be loaded.\")\n",
    "                    if seg_img is None:\n",
    "                        raise ValueError(f\"Segmentation image at {dp.seg_filepath} could not be loaded.\")\n",
    "\n",
    "                    # Convert from BGR (OpenCV default) to RGB (for matplotlib)\n",
    "                    image_rgb = np.array(rgb_img) \n",
    "                    if augmented_img is None:\n",
    "                        augmented_img = image_rgb\n",
    "                    augmented_img_rgb = np.array(augmented_img)\n",
    "\n",
    "                    # Create a new figure for each image\n",
    "                    plt.figure(figsize=(12, 8))  # Adjust figure size to make space for metadata and the new ROI subplot\n",
    "\n",
    "                    # Subplot for original RGB image\n",
    "                    plt.subplot(2, 3, 1)  # 2 rows, 3 columns, 1st subplot\n",
    "                    plt.imshow(image_rgb)\n",
    "                    plt.axis('off')  # Hide axes\n",
    "                    plt.title(f'Original Image {i}')\n",
    "\n",
    "                    # Subplot for augmented RGB image\n",
    "                    plt.subplot(2, 3, 2)  # 2 rows, 3 columns, 2nd subplot\n",
    "                    plt.imshow(augmented_img_rgb)\n",
    "                    plt.axis('off')  # Hide axes\n",
    "                    plt.title(f'Augmented Image {i}')\n",
    "\n",
    "                    # Subplot for segmentation image\n",
    "                    plt.subplot(2, 3, 3)  # 2 rows, 3 columns, 3rd subplot\n",
    "                    plt.imshow(seg_img, cmap='viridis')  # Use a colormap for better visualization\n",
    "                    plt.axis('off')  # Hide axes\n",
    "                    plt.title(f'Segmentation Image {i}')\n",
    "\n",
    "                    # Subplot for RGB image - keypoints\n",
    "                    keypoints_image = overlay_points_on_image(image=np.array(augmented_img_rgb), pixel_points=keypoints, radius=1)\n",
    "                    plt.subplot(2, 3, 4)  # 2 rows, 3 columns, 4th subplot\n",
    "                    plt.imshow(keypoints_image)\n",
    "                    plt.axis('off')  # Hide axes\n",
    "                    plt.title(f'Keypoints Image {i}')\n",
    "\n",
    "                    # Subplot for ROI image\n",
    "                    plt.subplot(2, 3, 5)  # 2 rows, 3 columns, 5th subplot\n",
    "                    plt.imshow(roi_image)\n",
    "                    plt.axis('off')  # Hide axes\n",
    "                    plt.title(f'ROI Image {i}')\n",
    "\n",
    "                    # Subplot for ROI image with keypoints \n",
    "                    roi_keypoints_image = overlay_points_on_image(image=np.array(roi_image), pixel_points=roi_keypoints, radius=1)\n",
    "                    plt.subplot(2, 3, 6)  # 2 rows, 3 columns, 6th subplot\n",
    "                    plt.imshow(roi_keypoints_image)\n",
    "                    plt.axis('off')  # Hide axes\n",
    "                    plt.title(f'ROI Keypoints Image {i}')\n",
    "                    \n",
    "                    # Display metadata as text in a separate area\n",
    "                    metadata_str = dp.__repr__()\n",
    "\n",
    "                    # Create a new subplot for metadata\n",
    "                    plt.text(1.05, 0.5, metadata_str, fontsize=12, ha='left', va='center', transform=plt.gca().transAxes,\n",
    "                            bbox=dict(facecolor='white', alpha=0.7, edgecolor='black', boxstyle='round,pad=1'))\n",
    "\n",
    "                    # Adjust layout to avoid overlap and make space for metadata\n",
    "                    plt.tight_layout()  # Adjust layout\n",
    "                    plt.subplots_adjust(right=0.8)  # Make space for metadata on the right\n",
    "\n",
    "                    # Save the image to the summary_images folder\n",
    "                    save_path = os.path.join(dataset_dir, \"summary_images\", f\"summary_image_{i}.png\")\n",
    "                    plt.savefig(save_path, bbox_inches='tight', dpi=300)  # Save with high resolution\n",
    "                    plt.close()  # Close the plot to free up memory\n",
    "\n",
    "\n",
    "                # Print progress every 10%\n",
    "                if i % (len(datapoints) / 10) == 0:\n",
    "                    print(f\"Processed {dataset_type} data: {i}/{len(datapoints)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints: 78947\n",
      "Processed 0 / 78947\n",
      "empty image at /home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250323-032914/rgb/rgb_4965.png\n",
      "empty image at /home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250323-032914/rgb/rgb_5519.png\n",
      "empty image at /home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250323-032914/rgb/rgb_6821.png\n",
      "empty image at /home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250323-032914/rgb/rgb_7641.png\n",
      "empty image at /home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250323-033133/rgb/rgb_392.png\n",
      "empty image at /home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250323-033133/rgb/rgb_1683.png\n",
      "empty image at /home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250324-183440/rgb/rgb_9304.png\n",
      "empty image at /home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250324-183440/rgb/rgb_16772.png\n",
      "empty image at /home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250324-183440/rgb/rgb_22683.png\n",
      "empty image at /home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250324-183440/rgb/rgb_31853.png\n",
      "empty image at /home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250324-183440/rgb/rgb_38281.png\n",
      "Number of filtered datapoints: 69897\n",
      "Processed train data: 0/55917\n",
      "Processed val data: 0/13979\n"
     ]
    }
   ],
   "source": [
    "data_folders = [\n",
    "    # \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250324-183440\",\n",
    "    # \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250324-181901\",\n",
    "    # \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250326-143252\",\n",
    "    # \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250323-032914\",\n",
    "    # \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250323-033133\",\n",
    "    # \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/markers_20250314-181037/\", # 1K dataset \n",
    "    # \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/markers_20250310-005350/\" # 72K dataset \n",
    "\n",
    "    \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250323-032914/\", \n",
    "    \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250323-033133/\", \n",
    "    \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250324-181901/\", \n",
    "    \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250324-183440/\", \n",
    "    \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250326-143252/\", \n",
    "    \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/sdg_markers_20250327-153817/\", \n",
    "\n",
    "    # \"/home/anegi/abhay_ws/marker_detection_failure_recovery/output/markers_20250310-005350/\",   \n",
    "    # \"/home/rp/abhay_ws/marker_detection_failure_recovery/test_data/markers_20250314-181037/\", # 1K dataset \n",
    "    # \"/home/rp/abhay_ws/marker_detection_failure_recovery/output/markers_20250320-133701/\" \n",
    "    # \"/home/rp/abhay_ws/marker_detection_failure_recovery/output/markers_20250310-005350/\" \n",
    "]\n",
    "\n",
    "# define OUT_DIR based on current date and time \n",
    "OUT_DIR = f\"/home/anegi/abhay_ws/marker_detection_failure_recovery/segmentation_model/data/data_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "# OUT_DIR = f\"/home/rp/abhay_ws/marker_detection_failure_recovery/test_data/data_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True) \n",
    "\n",
    "# Create an instance of the DataProcessor class\n",
    "processor = DataProcessor(data_folders, OUT_DIR)\n",
    "# define marker\n",
    "processor.set_marker(image_path=\"../synthetic_data_generation/assets/tags/4x4_1000-31.png\", num_squares=6, side_length=0.100) \n",
    "# processor.set_marker(image_path=\"../synthetic_data_generation/assets/tags/tag36h11_0.png\", num_squares=10, side_length=0.100) \n",
    "# Process the folders to create the datapoint list\n",
    "processor.process_folders()\n",
    "\n",
    "# Retrieve and print length of the datapoints before and after filtering \n",
    "print(f\"Number of datapoints: {len(processor.datapoints)}\") \n",
    "processor.filter_datapoints() \n",
    "print(f\"Number of filtered datapoints: {len(processor.datapoints_filtered)}\")  \n",
    "# Retrieve filtered datapoints\n",
    "datapoints = processor.get_datapoints_filtered()\n",
    "    \n",
    "# Split the datapoints into training and validation sets\n",
    "processor.split_train_val_data(filter=True, frac_train=0.8)\n",
    "processor.save_train_val_data(num_augmentations=1, save_summary_image=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marker_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
